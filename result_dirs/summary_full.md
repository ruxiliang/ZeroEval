| Model                            |   GSM8K |   MMLU<br/>-Redux |   ZebraLogic |   CRUX |   MATH<br/>-L5 |
|:---------------------------------|--------:|------------------:|-------------:|-------:|---------------:|
| o1-preview-2024-09-12            |  nan    |             92.84 |        71.40 |  95.88 |          84.47 |
| Llama-3.1-405B-Inst@hyperbolic   |   95.98 |             84.41 |       nan    |  73.50 |         nan    |
| o1-mini-2024-09-12               |   96.06 |             86.72 |        52.60 |  93.75 |          89.32 |
| claude-3-5-sonnet-20241022       |   96.66 |             88.91 |        36.20 |  83.88 |          59.36 |
| Llama-3.1-405B-Inst-fp8@together |   95.91 |             85.64 |        32.60 |  74.12 |         nan    |
| gpt-4o-2024-08-06                |   96.21 |             88.26 |        31.70 |  87.00 |          55.34 |
| gpt-4o-2024-05-13                |   95.38 |             88.01 |        28.20 |  86.12 |          54.79 |
| o1-preview-2024-09-12-v2         |  nan    |            nan    |        70.40 | nan    |         nan    |
| gemini-1.5-pro-exp-0801          |   95.00 |             85.53 |        25.20 |  74.88 |         nan    |
| claude-3-5-sonnet-20240620       |   95.60 |             86.00 |        33.40 |  80.75 |          51.87 |
| Llama-3.1-405B-Inst@sambanova    |   95.91 |             86.21 |        30.10 |  73.00 |          49.79 |
| deepseek-v2-chat-0628            |   93.93 |             80.81 |        22.70 |  70.50 |         nan    |
| Mistral-Large-2                  |   95.53 |             82.97 |        29.00 |  75.12 |          48.54 |
| gemini-1.5-pro-exp-0827          |  nan    |             86.14 |        30.50 |  79.62 |          68.10 |
| deepseek-v2-coder-0724           |   91.51 |             80.24 |        20.50 |  69.50 |         nan    |
| deepseek-v2-coder-0614           |   93.78 |             79.63 |        21.10 | nan    |         nan    |
| gpt-4o-mini-2024-07-18           |   94.24 |             81.50 |        20.10 |  75.88 |          52.15 |
| chatgpt-4o-latest-24-09-07       |  nan    |             88.88 |        29.90 |  86.50 |          53.12 |
| claude-3-sonnet-20240229         |   91.51 |             74.87 |        18.70 |  66.62 |         nan    |
| claude-3-opus-20240229           |   95.60 |             82.54 |        27.00 |  70.38 |          36.89 |
| deepseek-v2.5-0908               |   92.49 |             80.35 |        22.10 |  70.00 |          44.66 |
| Meta-Llama-3.1-70B-Instruct      |   94.16 |             82.97 |        24.90 |  64.25 |          43.13 |
| claude-3-5-haiku-20241022        |   94.47 |             79.63 |        18.70 |  68.75 |          46.46 |
| Qwen2.5-72B-Instruct             |  nan    |             85.57 |        26.60 |  73.88 |          60.19 |
| yi-large-preview                 |   82.64 |             82.15 |        18.90 |  60.38 |         nan    |
| gemini-1.5-pro                   |   93.40 |             82.76 |        19.40 |  68.00 |          39.81 |
| yi-large                         |   80.06 |             81.17 |        18.80 |  60.25 |         nan    |
| gpt-4-turbo-2024-04-09           |  nan    |             85.31 |        28.40 |  78.88 |          46.46 |
| o1-mini-2024-09-12-v3            |  nan    |            nan    |        59.70 | nan    |         nan    |
| gemini-1.5-flash-exp-0827        |  nan    |             82.11 |        25.00 |  74.50 |          54.51 |
| Qwen2-72B-Instruct               |   92.65 |             81.61 |        21.40 |  59.13 |          38.28 |
| gemini-1.5-flash                 |   91.36 |             77.36 |        19.40 |  63.75 |          34.81 |
| o1-mini-2024-09-12-v2            |  nan    |            nan    |        56.80 | nan    |         nan    |
| Llama-3-Instruct-8B-SimPO-v0.2   |   57.54 |             55.22 |       nan    | nan    |         nan    |
| Meta-Llama-3-70B-Instruct        |   93.03 |             78.01 |        16.80 |  58.88 |          25.10 |
| command-r-plus                   |   80.14 |             68.61 |        13.90 | nan    |         nan    |
| Mistral-Nemo-Instruct-2407       |   82.79 |             66.88 |        11.80 | nan    |         nan    |
| gemma-2-27b-it                   |   90.22 |             75.67 |        16.30 |  57.25 |          26.63 |
| gpt-4-0314                       |  nan    |             81.64 |        27.10 |  74.50 |          26.07 |
| Athene-70B                       |   86.66 |             76.64 |        16.70 |  50.62 |          20.67 |
| claude-3-haiku-20240307          |   88.78 |             72.32 |        14.30 |  54.75 |          15.12 |
| reka-core-20240501               |   87.41 |             76.42 |        13.00 |  46.25 |          21.91 |
| Qwen2.5-7B-Instruct              |  nan    |             75.13 |        12.00 |  52.75 |          51.46 |
| gemma-2-9b-it                    |   87.41 |             72.82 |        12.80 |  46.00 |          19.42 |
| Mixtral-8x7B-Instruct-v0.1       |   70.13 |             63.17 |         8.70 |  44.88 |         nan    |
| Yi-1.5-34B-Chat                  |   84.08 |             72.79 |        11.50 |  44.12 |          18.17 |
| reka-flash-20240226              |   74.68 |             64.72 |         9.30 |  34.12 |         nan    |
| gpt-3.5-turbo-0125               |   80.36 |             68.36 |        10.10 |  54.75 |          13.73 |
| Meta-Llama-3.1-8B-Instruct       |   84.00 |             67.24 |        12.80 |  39.88 |          22.19 |
| Phi-3-mini-4k-instruct           |   75.51 |             70.34 |        11.60 |  44.75 |          16.23 |
| Qwen2-7B-Instruct                |   80.06 |             66.92 |         8.40 |  37.88 |          23.86 |
| Phi-3.5-mini-instruct            |   82.03 |             67.67 |         6.40 |  42.12 |          18.72 |
| Qwen2-1.5B-Instruct              |   43.37 |             41.11 |       nan    | nan    |         nan    |
| Yi-1.5-9B-Chat                   |   76.42 |             65.05 |         2.30 |  44.75 |          19.97 |
| command-r                        |   52.99 |             61.12 |         9.90 | nan    |         nan    |
| Meta-Llama-3-8B-Instruct         |   78.47 |             61.66 |        11.90 |  37.75 |           7.91 |
| Rex-v0.1-1.5B                    |  nan    |             39.09 |       nan    | nan    |         nan    |
| Qwen2.5-3B-Instruct              |  nan    |             64.25 |         4.80 |  33.12 |          25.52 |
| gemma-2-2b-it                    |   51.63 |             51.94 |         4.20 |  21.50 |           4.30 |
| mathstral-7B-v0.1                |  nan    |            nan    |         9.00 | nan    |         nan    |
| Llama-3.2-3B-Instruct@together   |  nan    |            nan    |         7.40 | nan    |         nan    |